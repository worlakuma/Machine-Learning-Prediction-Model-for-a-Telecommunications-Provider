{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction and Lifetime Value Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<!-- TOC-->\n",
    "\n",
    "- [Step 1: Business Understanding](#step-1-business-understanding)\n",
    "  - [Scenario](#scenario)\n",
    "  - [Objective](#objective)\n",
    "  - [Hypothesis: ...](#hypothesis-)\n",
    "  - [Key Business Questions](#key-business-questions)\n",
    "  - [Approach](#approach)\n",
    "- [Step 2: Data Understanding](#step-2-data-understanding)\n",
    "  - [Project Initialisation](#project-initialisation)\n",
    "  - [Data Collection](#data-collection)\n",
    "    - [Access the LP2_Telco_churn_first_3000 data set from Microsoft SQL Server](#access-the-LP2_Telco_churn_first_3000-data-from-microsoft-sql-server)\n",
    "      - [Connect to the database using provided credentials](#connect-to-the-database-using-provided-credentials)\n",
    "      - [Fetch Information Schema for tables in the database](#fetch-information-schema-for-tables-in-the-database)\n",
    "      - [Load LP2\\_Telco\\_churn\\_first\\_3000 Data](#load-lp2_Telco_churn_3000-data)\n",
    "    - [Access the LP2_Telco_churn_second_2000 data from GitHub Repository](#access-the-LP2_Telco_churn_second_2000-data-from-github-repository)\n",
    "    - [Access the Telco-churn-last-2000 data from OneDrive](#access-Telco-churn-last-2000-data-from-onedrive)\n",
    "  - [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "- [Step 3: Data Preparation](#step-3-data-preparation)\n",
    "  - [Data Cleaning](#data-cleaning)\n",
    "  - [Data Transformation](#data-transformation)\n",
    "  - [Data Integration](#data-integration)\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "<!-- ## Worlflow\n",
    "![A beautiful sunset](https://example.com/sunset.jpg \"Sunset at the beach\") -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Step 1: Business Understanding\n",
    "\n",
    "### Background\n",
    "\n",
    "### Scenario\n",
    "\n",
    "### Objective\n",
    " \n",
    "### Hypothesis: Influence of Top Investors\n",
    "\n",
    "**A. Rationale:**\n",
    "\n",
    "**B. Null Hypothesis (H0):**\n",
    "\n",
    "**C. Alternative Hypothesis (H1):**\n",
    "\n",
    "### Key Business Questions\n",
    "\n",
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Step 2: Data Understanding\n",
    "\n",
    "`Data Collection`\n",
    "\n",
    "<!-- To effectively analyse the Indian start-up ecosystem from 2018 to 2021, comprehensive data collection is crucial. The data will be sourced from multiple datasets that detail startup funding activities within this period. Each dataset will encompass various aspects essential for a holistic understanding of the funding landscape. Specifically, the datasets will include: -->\n",
    "\n",
    "**A. Feature Details:**\n",
    "\n",
    "**B. ....:**\n",
    "\n",
    "`Data Cleaning`\n",
    "\n",
    "Effective data cleaning is crucial to ensure the integrity and usability of the dataset for analysis. The steps involved in data cleaning will include:\n",
    "\n",
    "**A. Remove Duplicates:**\n",
    "\n",
    "Identify and remove duplicate records to ensure that each entry is unique and not counted multiple times. \n",
    "\n",
    "**B. Eliminate Irrelevant Data:**\n",
    "\n",
    "Remove any data that is not relevant to the analysis. This could include fields that do not contribute to the understanding of customer churns or other extraneous information.\n",
    "\n",
    "\n",
    "`Exploratory Data Analysis`\n",
    "\n",
    "Inspect the dataset in depth, visualise it to answer analytical questions and plan the cleaning, processing and feature creation.\n",
    "\n",
    "**A. Univariate Analysis:**\n",
    "Explore, analyze and visualize key variables independently of others\n",
    "\n",
    "**B. Bivariate Analysis:**\n",
    "Explore, analyze and visualize the relationship among the variables\n",
    "\n",
    "**C. Deploy the visualisations to Power Bi:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing necessary libraries\n",
    "# #from dotenv import dotenv_values\n",
    "# # import pyodbc\n",
    "#import requests\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy.stats import skew, kurtosis\n",
    "# from fancyimpute import IterativeImputer\n",
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# from sklearn.impute import IterativeImputer\n",
    "# import re\n",
    "# import calendar\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "\n",
    "# # Suppressing all warnings to avoid cluttering the output\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# # Set display options for Pandas DataFrame\n",
    "# pd.set_option(\"display.max_rows\", 100)\n",
    "# pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Step 3: Data Preparation & Feature Engineering\n",
    "\n",
    "`Data Splitting`\n",
    "\n",
    "Use train_test_split with a random_state, and add stratify for Classification\n",
    "\n",
    "`Impute Missing Values`\n",
    "\n",
    "Use sklearn.impute.SimpleImputer\n",
    "\n",
    "`Features Creation`\n",
    "\n",
    "\n",
    "`Features Encoding`\n",
    "From sklearn.preprocessing use OneHotEncoder to encode the categorical features.\n",
    "\n",
    "`Feature Scaling`\n",
    "\n",
    "Use sklearn.preprocessing.StandardScaler to scale the numerical features.\n",
    "\n",
    "`Train set Balancing`\n",
    "\n",
    "Use sklearn.utils.resample to balance the dataset and or use Over-sampling/Under-sampling methods, more details here: https://imbalanced-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Step 4: Modeleling and Evaluation\n",
    "Build, train, and test the four(4)models and models.\n",
    "\n",
    "`Craete the Model`\n",
    "\n",
    "`Train the Model`\n",
    "use the following method to train the model: .fit() method\n",
    "\n",
    "`Evaluate the Model on the Evaluation dataset`\n",
    "Compute the valid metrics for the model\n",
    "\n",
    "`Predict on the unknown dataset`\n",
    "Use .predict method .predict_proba()\n",
    "\n",
    "`Save the Model`\n",
    "\n",
    "`repeat the above method by creating three(3)models`\n",
    "\n",
    "`Model Comparison`\n",
    "\n",
    "`Hyperparameter Analysis`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents)\n",
    "\n",
    "## Step 6: Document the entire process in an article on medium"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
